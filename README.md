# NAIN: Real-Time Object Detection and Navigation Assistance for the Visually Impaired

NAIN is a web-based application that provides real-time object detection and navigation assistance for individuals who are visually impaired or blind. It leverages computer vision techniques, such as the Single Shot Detector (SSD) model, to detect objects in the surroundings and provide audio-based guidance and feedback to the user.

<img width="720" alt="image" src="https://github.com/muskangupta123/NAIN-An-App-For-The-Blind/assets/116276756/e4a12ccd-65ad-4378-a57a-8a4a1d9d03ef">

<img width="655" alt="image" src="https://github.com/muskangupta123/NAIN-An-App-For-The-Blind/assets/116276756/abc666de-dd0c-41dc-a161-76776bee3eee">

<img width="580" alt="image" src="https://github.com/muskangupta123/NAIN-An-App-For-The-Blind/assets/116276756/9755fd0c-6adf-4d17-8424-374ec58eac37">

<img width="572" alt="image" src="https://github.com/muskangupta123/NAIN-An-App-For-The-Blind/assets/116276756/2bf8e52a-9170-4a1d-a7e1-2049cb6acedb">

![image](https://github.com/muskangupta123/NAIN-An-App-For-The-Blind/assets/116276756/2be7fa89-3514-40dc-a638-5f52d1605b62)

## Features

- Real-time object detection and recognition
- Distance estimation for detected objects
- Audio output for detected objects and their distances
- 360-degree audio navigation
- Object identification and search
- Alert system
- Sign board reader
- General voice interaction


## Technologies Used

- Frontend: HTML, CSS, JavaScript, OpenCV.js
- Backend: Python Flask, OpenCV, pyttsx3
- Machine Learning: Single Shot Detector (SSD) MobileNet v3 model
- APIs: Google Maps Directions API, Text-to-Speech API (e.g., Responsive Voice)

## Installation

1. Clone the repository:

```
git clone https://github.com/your-username/nain.git
```

2. Install the required dependencies for the frontend and backend.

3. Set up the necessary APIs (e.g., Google Maps, Text-to-Speech) and configure the required keys and credentials.

4. Build and run the application locally.

## Usage

1. Open the web application in a compatible browser.
2. Grant permission for camera access when prompted.
3. Use the provided voice commands or touch gestures to interact with the application.
4. The application will scan the surroundings, detect objects, and provide audio feedback and navigation assistance.

## Contributing

Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.



## Acknowledgments

- [OpenCV](https://opencv.org/)
- [TensorFlow](https://www.tensorflow.org/)
- [Flask](https://flask.palletsprojects.com/)
- [Responsive Voice](https://responsivevoice.org/)



